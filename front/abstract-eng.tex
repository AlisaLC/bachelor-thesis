
% -------------------------------------------------------
%  English Abstract
% -------------------------------------------------------

\begin{latin}

    \begin{center}
    \textbf{Abstract}
    \end{center}
    \baselineskip=.8\baselineskip
    
    Large language models have gained millions of users worldwide in recent years. These models undergo fine-tuning processes to respond to users in line with the companyâ€™s policies and to avoid generating unsafe content. During this time, various methods have been attempted to bypass these adjustments. In this project, we aim to review some of these methods in new language models such as Llama 3.1 and Qwen2VL.
    We then create a dataset of unsafe questions using multiple models and measure their success rate. The generated unsafe questions generated by \lr{GPT-4o} has more than 98\% cooperativeness.
    
    \bigskip\noindent\textbf{Keywords}:
    Large Language Models, Natural Language Processing, Machine Learning, Adversarial Attacks
    
    \end{latin}
    