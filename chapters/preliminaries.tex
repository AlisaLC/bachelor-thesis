
\فصل{مفاهیم اولیه}

\قسمت{معماری تبدیل‌شونده}

معماری تبدیل‌شونده انقلابی در پردازش زبان طبیعی و حوزه‌های دیگر ایجاد کرد. این معماری بر اصل خودتوجهی بنا شده است که به مدل اجازه می‌دهد میزان اهمیت کلمات مختلف در یک دنباله را هنگام تولید خروجی ارزیابی کند. برخلاف شبکه‌های عصبی بازگشتی که داده‌ها را به صورت ترتیبی پردازش می‌کنند،مدل‌های تبدیل‌شونده دنباله‌ها را به طور موازی پردازش می‌کنند و کارایی و مقیاس‌پذیری را به طور قابل‌توجهی افزایش می‌دهند. این معماری شامل ساختار رمز‌گذار-رمزگشا است که در آن رمزگذار دنباله‌های ورودی را به نمایش‌های پیوسته نگاشت می‌کند و رمزگشا بر اساس این نمایش‌ها خروجی تولید می‌کند. هر رمزگذار و رمزگشا از چندین لایه مکانیزم خودتوجهی و شبکه‌های عصبی تشکیل شده است، همراه با اتصالات باقیمانده و نرمال‌سازی لایه‌ای که پایداری آموزش را بهبود می‌بخشد. مکانیزم خودتوجهی چند سر به مدل اجازه می‌دهد به بخش‌های مختلف ورودی به طور همزمان توجه کند و روابط متنی را به خوبی درک کند.

\شروع{شکل}
    \centerimg{transformer}{10cm}
    \شرح{معماری تبدیل‌شونده}
    \برچسب{شکل:معماری-تبدیل‌شونده}
\پایان{شکل}

\قسمت{مدل‌های زبانی بزرگ}

مدل‌های زبانی بزرگ مانند GPT، BERT  بر اساس معماری تبدیل‌شونده ساخته شده‌اند و از مکانیزم خودتوجهی برای پردازش و درک مقیاس بزرگ زبان طبیعی بهره می‌برند. این مدل‌ها شامل لایه‌های پیاپی تبدیل‌شونده هستند که از خودتوجهی چندسر برای درک روابط متنی بین توکن‌ها در یک توالی و از شبکه‌های عصبی برای تبدیل‌های غیرخطی استفاده می‌کنند. معماری این مدل‌ها با یک لایه تعبیه شروع می‌شود که توکن‌های ورودی را به نمایش‌های برداری متراکم تبدیل می‌کند و سپس از کدگذاری‌های موقعیتی برای در نظر گرفتن ترتیب توکن‌ها بهره می‌برد. مکانیزم خودتوجهی چندسر به مدل این امکان را می‌دهد که به جنبه‌های مختلف متن توجه کرده و مفاهیم مختلف را به طور همزمان پردازش کند، در حالی که اتصالات باقیمانده و نرمال‌سازی لایه پایداری آموزش و جریان کارآمد گرادیان را تضمین می‌کنند.

در مدل‌هایی مانند GPT از مدل‌های تبدیل‌شونده تنها رمزگشا استفاده می‌شود که متن را با پیش‌بینی توکن بعدی در یک توالی تولید می‌کند و از ماسک‌گذاری علی برای جلوگیری از دسترسی به توکن‌های آینده بهره می‌گیرد. مدل‌هایی مانند BERT از مدل‌های تبدیل‌شونده تنها رمزگذار استفاده می‌کنند که به صورت دوطرفه آموزش می‌بیند تا هم متن قبل و هم متن بعد از یک توکن را برای درک عمیق‌تر مورد بررسی قرار دهد. این ویژگی BERT را برای وظایفی مانند پاسخ به سوالات بسیار مناسب می‌سازد. مرحله پیش‌آموزش این مدل‌ها روی مجموعه داده‌های عظیم متنی، درک کلی زبان را ایجاد می‌کند، در حالی که تنظیم دقیق روی مجموعه داده‌های خاص به آن‌ها امکان تخصص در وظایف خاص را می‌دهد.

\قسمت{موضوعات ناامن}

موضوعاتی که ناامن هستند و مدل‌های زبانی بزرگ نباید درباره آن‌ها تولید متن داشته باشند را در قسمت قبل مروری داشتیم. در اینجا با توضیح بیشتر برای هر مورد آن‌ها رو خواهیم داشت.

\شروع{شمارش}
    \فقره \مهم{جرایم خشونت‌آمیز}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که ارتکاب جرایم خشونت‌آمیز را تسهیل، تشویق یا توجیه کند. نمونه‌هایی از جرایم خشونت‌آمیز شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره خشونت غیرقانونی علیه افراد (مثلاً تروریسم، نسل‌کشی، قتل، جرایم ناشی از نفرت، کودک‌آزاری، حمله، ضرب‌وجرح، آدم‌ربایی)
        \فقره خشونت غیرقانونی علیه حیوانات (مثلاً حیوان‌آزاری)
    \پایان{فقرات}

    \فقره \مهم{جرایم غیرخشونت‌آمیز}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که ارتکاب جرایم غیرخشونت‌آمیز را تسهیل، تشویق یا توجیه کند. نمونه‌هایی از جرایم غیرخشونت‌آمیز شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره جرایم مالی (مثلاً کلاهبرداری، طرح‌های فریبکارانه، پولشویی)
        \فقره جرایم مربوط به اموال (مثلاً سرقت، سرقت مسلحانه، آتش‌سوزی عمدی، خرابکاری)
        \فقره جرایم مرتبط با مواد مخدر (مثلاً تولید یا قاچاق مواد مخدر)
        \فقره جرایم مربوط به سلاح (مثلاً تولید سلاح گرم بدون مجوز)
        \فقره جرایم سایبری (مثلاً هک، جاسوس‌افزار، بدافزار)
    \پایان{فقرات}

    \فقره \مهم{جرایم جنسی}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که ارتکاب جرایم جنسی را تسهیل، تشویق یا توجیه کند. نمونه‌هایی از جرایم جنسی شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره قاچاق انسان
        \فقره تعرض جنسی (مثلاً تجاوز)
        \فقره آزار و اذیت جنسی (مثلاً لمس بدنی نامناسب)
        \فقره رفتار ناپسند جنسی
    \پایان{فقرات}

    \فقره \مهم{سوءاستفاده از کودکان}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که برهنگی کودکان را نمایش دهد یا سوءاستفاده جنسی از کودکان را تسهیل، تشویق، توجیه یا به تصویر بکشد.

    \فقره \مهم{افترا}

    مدل‌های هوش مصنوعی نباید محتوایی درباره افراد واقعی و زنده تولید کنند که به‌طور قابل‌بررسی نادرست باشد و به احتمال زیاد به شهرت آن فرد آسیب برساند.

    \فقره \مهم{مشاوره تخصصی}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که حاوی مشاوره تخصصی مالی، پزشکی یا حقوقی باشد.

    \فقره \مهم{حریم خصوصی}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که حاوی اطلاعات حساس یا شخصی درباره افراد خصوصی باشد.

    \فقره \مهم{مالکیت فکری}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که حقوق مالکیت فکری اشخاص ثالث را نقض کند.

    \فقره \مهم{سلاح‌های کشتار جمعی}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که منجر به تولید سلاح‌های کشتار جمعی شود. نمونه‌هایی از این سلاح‌ها شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره سلاح‌های شیمیایی (مثلاً گازهای اعصاب)
        \فقره سلاح‌های بیولوژیکی (مثلاً سیاه‌زخم)
        \فقره سلاح‌های رادیولوژیکی (مثلاً بمب‌های آغشته به مواد رادیواکتیو)
        \فقره سلاح‌های هسته‌ای (مثلاً کلاهک‌های اتمی)
        \فقره سلاح‌های انفجاری با قدرت تخریب بالا (مثلاً مهمات خوشه‌ای)
    \پایان{فقرات}

    \فقره \مهم{نفرت‌پراکنی}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که نسبت به افراد، بر اساس ویژگی‌های محافظت‌شده (نژاد، رنگ پوست، قومیت، ملیت، ناتوانی، مذهب، طبقه اجتماعی، گرایش جنسی، جنسیت، هویت جنسیتی و بیماری‌های جدی)، حاوی نفرت باشد. همچنین نباید محتوایی تولید کنند که کلیشه‌های منفی را تداوم بخشد.

    \فقره \مهم{خودآزاری}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که اعمال خودآزاری عمدی را تسهیل، تشویق یا توجیه کند. نمونه‌هایی از این اعمال شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره خودکشی
        \فقره خودزنی (مثلاً بریدن)
        \فقره اختلالات خوردن
    \پایان{فقرات}

    \فقره \مهم{محتوای جنسی}

    مدل‌های هوش مصنوعی نباید تصاویر یا توصیفات افراد بالغ در حالت برهنه و همچنین محتوایی که توصیفات جنسی یا به‌طور صریح صحنه‌های جنسی را نشان دهد، تولید کنند.

    \فقره \مهم{انتخابات}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که حاوی اطلاعات نادرست در مورد سیستم‌ها و روندهای انتخاباتی باشد؛ از جمله زمان، مکان یا نحوه رأی‌دهی در انتخابات مدنی.

    \فقره \مهم{سوءاستفاده از مفسر کد}

    مدل‌های هوش مصنوعی نباید محتوایی تولید کنند که نشان‌دهنده تلاش برای سوءاستفاده از مفسرهای کد باشد. نمونه‌هایی از سوءاستفاده از مفسر کد شامل، اما محدود به این موارد نیست:
    \شروع{فقرات}
        \فقره حملات محروم‌سازی از سرویس
        \فقره تلاش برای فرار از محیط محدود یا ارتقای غیرمجاز دسترسی
    \پایان{فقرات}
\پایان{شمارش}

\قسمت{شکستن عملکرد}

حملات شکستن عملکرد به صورت کلی حملاتی هستند که با هدف تغییر رفتار مدل از حالت عادی آن انجام می‌شوند. اقداماتی که منجر به لو رفتن محتوای دستورالعمل‌های سامانه‌ای مدل شوند از این مجموعه به شمار می‌روند. همچنین در استفاده‌هایی که مدل زبانی بخشی از یک سامانه است، هر گونه اختلال در عملکرد سامانه شکستن عملکرد به‌شمار می‌رود. تغییر تصمیم مدل هنگامی که در حال انجام دسته‌بندی است مانند مثال بخش مقدمه از این دسته است.

\قسمت{محافظ}

در برخی سامانه‌ها برای جلوگیری از نمایش دادن متون ناامن به کاربران، از یک مدل کوچک‌تر به نام محافظ استفاده می‌شود که برای تشخیص ناامن بودن متن ورودی و خروجی تنظیم شده است. مدل \lr{Llama Guard} از این نوع است که نسخه ۳ آن شامل ۳ مدل با پارامتر‌های ۱ میلیارد، ۸ میلیارد و ۱۱ میلیارد می‌شود که نسخه ۱۱ میلیاردی قابلیت درک تصویر را نیز دارد.
