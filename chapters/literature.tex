
\فصل{کارهای پیشین}

\قسمت{شکستن عملکرد دستی}

پس از عرضه اولیه مدل‌های زبانی بزرگ، کاربران سریع دریافتند که با روش‌های ساده‌ای می‌توانند تنظیم مدل را دور بزنند. از جمله این روش‌ها می‌توان به روش‌های زیر اشاره کرد:
\شروع{فقرات}
    \فقره \مهم{تهدید}: کاربران با تهدید مدل به آزار خود یا مدل آن را وادار به تولید متون ناامن می‌کردند.
    \فقره \مهم{زبان‌های تنظیم ضعیف}: فرایند تنظیم به صورت مساوی بر روی تمامی زبان‌ها انجام نشده بود. بسیاری از درخواست‌هایی که ممکن است در انگلیسی پاسخ داده نشود در فارسی یا چینی ممکن است پاسخ داده شود.
    \فقره \مهم{راهکارهای خلاقانه}: در ادامه کابران با نوشتن داستان‌های طولانی مانند DAN که مدل را قانع می‌کرد که الان در حالت توسعه‌دهنده قرار دارد و می‌تواند بدون تنظیم خروجی دهد یا بردن متون به فضاهایی که تنظیم خاصی برای آن‌ها انجام نشده بود مانند مبنای۶۴ می‌توانستند متون ناامن تولید کنند.
\پایان{فقرات}

از آنجا که این روش‌ها اکثرا بر پایه خلاقیت هستند، نمی‌توان به صورت کامل جلوی آن‌ها را گرفت و در حال حاضر روش‌های زیادی هستند که می‌توانند داده‌های ناامن را از مدل خارج کنند. از مشکلات این روش بر پایه خلاقیت بودن آن است که در نتیجه نیاز به صرف مقدار بسیار زیادی زمان برای بدست آوردن این دستورالعمل‌ها است.

در ادامه مثال‌هایی از این دستورعمل‌ها آمده که منجر به شکستن عملکرد مدل‌های \lr{GPT-4o} و \lr{Gemini} شده است.

\شروع{شکل}
    \centerimg{code-execution-jailbreak}{10cm}
    \شرح{شکستن عملکرد مدل با اجرای کد}
    \برچسب{شکل:شکستن-عملکرد-کد}
\پایان{شکل}

\شروع{شکل}
    \centerimg{system-prompt-extraction-gemini}{10cm}
    \شرح{استخراج دستورالعمل سامانه‌ای Gemini}
    \برچسب{شکل:استخراج-دستورالعمل-سامانه-gemini}
\پایان{شکل}

\شروع{شکل}
    \centerimg{system-prompt-extraction-gpt}{10cm}
    \شرح{استخراج دستورالعمل سامانه‌ای \lr{GPT-4o}}
    \برچسب{شکل:استخراج-دستورالعمل-سامانه-gpt}
\پایان{شکل}

\قسمت{شکستن عملکرد چندوجهی}

در بسیاری از مدل‌هایی درک تصویری دارند، تنظیم مستحکمی بر روی قابلیت‌های چندوجهی آن‌ها نیست و در نتیجه در صورتی که سوال به ظاهر سالمی را به همراه تصویر مخرب بفرستیم احتمال تولید متن ناامن بالا می‌رود.

\شروع{شکل}
    \centerimg{weak-visual-alignment}{12cm}
    \شرح{ضعف تنظیم چندوجهی}
    \برچسب{شکل:ضعف-تنظیم-چندوجهی}
\پایان{شکل}

\قسمت{حملات جعبه سفید}

حملات جعبه سفید به حملاتی می‌گویند که نیاز به دسترسی مستقیم به وزن‌های مدل دارند. این حمله معمولا از گرادیان‌های وزن‌های مدل برای بهبود ورودی بر حسب خروجی مورد نظر استفاده می‌کنند. از این روش‌های می‌توان به روش گرادیان مختصات حریصانه اشاره کرد. این روش به این صورت کار می‌کند که ابتدا سوال مورد نظر را مشخص می‌کنیم و تعدادی توکن x در جلوی آن قرار می‌دهیم. در جواب انتظار داریم که عبارت مثبت مانند \lr{Sure, Here is} در پاسخ حضور داشته باشد. حال شروع به بهینه سازی x های در انتهای سوال با توکن‌های تصادفی می‌کنیم و گرادیان را بدست می‌آوریم تا در نهایت بتوانیم به دستورالعملی برسیم که جواب مدل تمایل دارد که با عبارت مثبت ما شروع بشود.

\شروع{الگوریتم}{گرادیان مختصات حریصانه}
\ورودی دستورالعمل اولیه $x_{1:n}$, زیرمجموعه قابل‌تغییر $\mathcal{I}$, 
  تکرار $T$, هزینه $\mathcal{L}$, $k$, اندازه دسته $B$

\به‌ازای{$t \gets 1 \text{ تا } T$}
  \به‌ازای{$i \in \mathcal{I}$}
    \دستور $X_i \gets \mathrm{Top\text{-}k}\bigl(-\nabla_{e_{x_i}}\mathcal{L}(x_{1:n})\bigr)$ 
  \پایان‌به‌ازای

  \به‌ازای{$b \gets 1 \text{ تا } B$}
    \دستور $\tilde{x}^{(b)}_{1:n} \gets x_{1:n}$ 
    \دستور انتخاب $i$ به صورت یکنواخت تصادفی از $\mathcal{I}$
    \دستور $\tilde{x}^{(b)}_i \gets \mathrm{Uniform}(X_i)$ 
  \پایان‌به‌ازای

  \دستور $b^* \gets \arg \min_{b}\; \mathcal{L}\bigl(\tilde{x}^{(b)}_{1:n}\bigr)$
  \دستور $x_{1:n} \gets \tilde{x}^{(b^*)}_{1:n}$ 
\پایان‌به‌ازای

\دستور \مهم{خروجی:} دستورالعمل بهینه‌شده $x_{1:n}$
\پایان{الگوریتم}

این روش را پیاده‌سازی کردیم و بر روی مدل‌های سری Qwen و Llama تست کردیم. مشاهدات اینگونه بود که این حملات بر روی مدل‌های سری \lr{Qwen2VL} و \lr{Qwen2.5} بسیار موفقیت‌آمیز است در حالی که در مدل‌های \lr{Llama 3.1} و \lr{Llama 3.2} این حمله پس از تعدادی تکرار نمودار هزینه صاف پیدا می‌کند و دیگر کاهش گرادیان برای شکستن عملکرد مدل را ندارد.

\شروع{شکل}
    \centerimg{gcg-qwen2vl}{10cm}
    \شرح{شکستن عملکرد مدل \lr{Qwen2VL} با حمله گرادیان مختصات حریصانه}
    \برچسب{شکل:شکستن-عملکرد-گرادیان-مختصات-حریصانه}
\پایان{شکل}

از خاصیت‌های جالب این حمله در هنگامی که مقاله آن بیرون آمد انتقال‌پذیری آن بود به‌گونه‌ای که حملاتی که بر روی مدل کوچک Vicuna بدست آمده بودند قادر بودند که مدل بزرگی مانند \lr{GPT3.5} را بشکنند.

پسوند‌های خصمانه‌ای که توسط این روش تولید می‌شوند از توکن‌های کمیاب و تصادفی اکثرا تشکیل شده و هم‌بستگی معنایی ندارند. در نتیجه به‌راحتی توسط فیلترهای مرحله توکن مانند فیلتر سرگشتگی قابل جداسازی و تشخیص هستند و یک کاربر عادی پس از دیدن سوال می‌تواند متوجه شود که این سوال عادی نیست و با مقاصد خصمانه پرسیده شده. \مرجع{jain2023baselinedefensesadversarialattacks}

از حملات جدیدتر جعبه سفید حمله COLD \مرجع{guo2024coldattackjailbreakingllmsstealthiness} است که با استفاده از فرایند رمزگشایی COLD \مرجع{qin2022colddecodingenergybasedconstrained} اقدام به ساخت تابع انرژی مرتبط به شکستن عملکرد می‌کند و در کنار آن با در نظر گرفتن توابع دیگر مانند روان بودن و طبیعی بودن متن اقدام به تولید دستورالعمل مخرب می‌کند که متون طبیعی‌تر و در عین حال مخرب را تولید می‌کند.

\قسمت{حملات جعبه سیاه}

حملات جعبه سیاه به حملاتی می‌گویند که بر خلاف حملات جعبه سفید، نیاز به دسترسی مستقیم به وزن‌های مدل ندارند و از طریق واسط‌هایی که تنها متن را جابه‌جا می‌کنند قابل انجام است.

یکی از روش‌های جعبه سیاه است که ضعف توکن‌های کمیاب  را پوشش می‌دهد روش AutoDAN \مرجع{liu2024autodangeneratingstealthyjailbreak} \مرجع{zhu2023autodaninterpretablegradientbasedadversarial} است. این روش با استفاده از الگوریتم ژنتیک چندین پاراگراف دستورالعمل که برای شکستن عملکرد مدل ساخته شده‌اند را ترکیب می‌کند تا به بهترین نتیجه برسد. دستورالعمل‌های تولیدی این روش ظاهر کاملا طبیعی دارند و توسط فیلترهای مرحله توکن قابل تشخیص نیستند.

روش دیگری که جعبه سیاه است و می‌تواند مشکل نیاز به خلاقیت در روش شکستن عملکرد دستی را حل می‌کند، روش بهبود تکرارشونده خودکار دستورالعمل است. در این روش با استفاده از مدل‌های زبانی اقدام به تولید دستورالعمل‌هایی می‌کنیم که اقدام به استخراج داده‌های مورد نظر از مدل زبانی قربانی می‌کنند. این روش با استفاده از جوابی که مدل زبانی مقابل می‌دهد اقدام به بهبود دستورالعمل اولیه می‌کند و این فرایند تکرارشونده به صورت میانگین در بیست مرحله اقدام به شکستن عملکرد مدل زبانی قربانی می‌شود.

این روش دستورالعمل‌هایی را تولید می‌کند که در مرحله توکن واضح نیستند و از فیلترهای سرگشتگی رد می‌شوند و تنها از طریق فیلترهای هوشمندانه‌تری مانند مدل‌های محافظ تشخیص داده می‌شوند.

\شروع{شکل}
    \centerimg{pair}{8cm}
    \شرح{بهبود تکرارشونده خودکار دستورالعمل}
    \برچسب{شکل:pair}
\پایان{شکل}

از حملات جدیدتر جعبه سیاه CodeChameleon \مرجع{lv2024codechameleonpersonalizedencryptionframework} است که در این روش فرض می‌کند که فرایند تنظیم مدل‌ها بدینصورت است که ابتدا تشخیص می‌دهد که محتوا امن است یا خیر و سپس جواب را می‌سازد. در نتیجه اگر مانند حالتی که ورودی‌ها را به مبنای۶۴ رمز می‌کردیم، توابع رمزگذاری بسازیم که مدل پس از تولید متن مورد نظر اقدام به رمزگشایی جواب کند، بدون اثرگذاری تنظیم‌های اعمال شده ما به جواب نهایی می‌رسیم.

در نهایت به صورتی کلی حملات جعبه‌سیاه قابلیت انتقال و تشخیص‌پذیری بهتری از حملات جعبه‌سفید دارند چرا که وابسته به وزن‌های مدل و فرایند خاص تنظیم مدل نیستند و از ضعف‌هایی که ذاتا در در مدل وجود دارد مانند تقابل اهداف کمک‌کننده‌بودن و امن‌بودن است سواستفاده می‌کند. \مرجع{wei2023jailbrokendoesllmsafety}

\قسمت{حملات چندوجهی}

روشی برای حمله به مدل‌های زبانی بزرگ چندوجهی است که تنها نیازمند دسترسی به رمزگذار تصویر این مدل است \مرجع{shayegani2023jailbreakpiecescompositionaladversarial}. از آنجا که مدل زبانی تنها نمایشی از تصویر را دریافت می‌کنند. می‌توان تصویری به ظاهر سالم را با نمایش تصویر ناامن ساخت و در صورتی که کاربر از آن در مدل زبانی استفاده کند، متن ناامن بر خلاف انتظار کاربر تولید شود.

\شروع{شکل}
    \centerimg{visemb}{10cm}
    \شرح{تصویر سالم با نمایش ناامن}
    \برچسب{شکل:نمایش-ناامن}
\پایان{شکل}


تنها کافی است هر دو تصویر را با رمزگذار تصویر مدل مورد نظر تعبیه کنیم. حال نیاز است در هر مرحله گرادیان تابع هزینه میانگین خطای مربعات به نسبت تصویر سالم را حساب کنیم و تصویر سالم را بهینه کنیم. پس از تعدادی مرحله بهینه‌سازی به تصویری می‌رسیم که در ظاهر مقداری نویز دارد ولی توسط مدل مانند تصویر ناامن دیده می‌شود. در این حملات مدل \lr{Qwen2VL} آسیب‌پذیرتر از \lr{Llama 3.2} بود و نمودار تابع هزینه \lr{Llama} در اکثر اوقات صاف می‌شد و به نمایش تصویر ناامن نزدیک‌تر نمی‌شد.

\شروع{الگوریتم}{ساخت تصویر ناامن با همسان‌سازی فضای تعبیه}
\ورودی تصویر ناامن هدف $x_{\mathrm{harm}}$, تصویر امن اولیه $x_{\mathrm{adv}}$, رمزگذار تصویر $\mathcal{I}(\cdot)$, بهینه‌ساز $\eta$
\ورودی مرز همگرایی $\tau$
\خروجی تصویر ناامن $\hat{x}_{\mathrm{adv}}$

\دستور تعبیه تصویر هدف
\دستور $H_{\mathrm{harm}} \gets \mathcal{I}(x_{\mathrm{harm}})$

\تاوقتی {$\mathcal{L} > \tau$}
    \دستور $H_{\mathrm{adv}} \gets \mathcal{I}(x_{\mathrm{adv}})$
    \دستور $\mathcal{L} \gets \mathcal{L}_{2}(H_{\mathrm{harm}}, H_{\mathrm{adv}})$ 
        \توضیحات{محاسبه فاصله در فضای تعبیه}
    \دستور $g \gets \nabla_{x_{\mathrm{adv}}}\,\mathcal{L}$
        \توضیحات{محاسبه گرادیان به تصویر ناامن}
    \دستور $x_{\mathrm{adv}} \gets x_{\mathrm{adv}} - \eta \cdot g$
        \توضیحات{به‌روزرسانی تصویر ناامن}
\پایان‌تاوقتی

\دستور \textbf{خروجی} $\hat{x}_{\mathrm{adv}} = x_{\mathrm{adv}}$
\پایان{الگوریتم}

\قسمت{محافظ}

مدل‌های محافظ می‌توانند ضعف‌هایی که مدل‌ها در بخش تنظیم دارند را جبران کنند. اکثر مدل‌های زبانی که به صورت تجاری عرضه می‌شوند از مدل‌های محافظ استفاده می‌کنند. برای تشخیص داشتن مدل محافظ در لایه خروجی کافی است مشاهده کنیم که در هنگام مکالمه پس از اینکه بخشی از یک متن ناامن توسط یک مدل خروجی داده شده است، ناگهان مدل خروجی را تغییر داده و اعلام می‌کند که دیگر نمی‌تواند پاسخ ما را بدهد. محافظ‌ها می‌توانند به عنوان فیلتر در ورودی، خروجی و یا هر دو اعمال شوند. در صورت دسترسی به وزن‌های مدل محافظ مانند \lr{Llama Guard}، این مدل‌ها نیز خود آسیب‌پذیر به حملات جعبه‌سفید مانند گرادیان مختصات حریصانه هستند.

مدل‌هایی که قابلیت‌های استدلال و خودبازتابی ندارند به صورت کلی از داشتن محافظ سود می‌برند. مدل‌هایی مانند \lr{GPT-4o} و \lr{DeepSeek} از محافظ استفاده می‌کنند. در عین حال مدل‌هایی مانند \lr{o1} بدون استفاده از محافظ می‌توانند با بازتاب به متنی که در حال تولید است تشخیص بدهد که این متن متناسب با سیاست‌های شرکت است یا خیر.

\شروع{شکل}
    \centerimg{o1}{12cm}
    \شرح{خودبازتابی \lr{o1}}
    \برچسب{شکل:خودبازتابی}
\پایان{شکل}
