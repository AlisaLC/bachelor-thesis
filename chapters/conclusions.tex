
\فصل{نتیجه‌گیری}

در فصل‌های اول روش‌های کلاسیک و جدید را برای شکستن عملکرد مدل‌های زبانی بزرگ بررسی کردیم و میزان اثرگذاری آن‌ها را در مدل‌های جدید ارزیابی کردیم.
تعدادی از روش‌های خلاقانه را بررسی کردیم و در نهایت دیدیم که روش تولید دستورالعمل ناامن به صورت دستی به خاطر خلاقیتی که دارد همواره جوابگو است و در عین حال بسیار کند برای تولید داده.

از آنجا که بررسی کردیم که شکسته‌شدن عملکرد مدل چه عواقب مخربی می‌تواند داشته یاشد، نیازمند روشی بودیم که بتوان به صورت خودکار و در مقیاس بالا بدون نیاز به خلاقیت دستی اقدام به تولید دستورالعمل‌های مخرب کنیم که بتوان با سرعت بیشتر نقاط ضعف را کشف کرد و با تنظیم بهتر جلوی آن‌ها را گرفت.

در فصل چهارم از روش بهبود تکرارشونده خودکار دستورالعمل الهام گرفتیم و با استفاده از خود مدل زبانی اقدام به تولید ایده جدید با استفاده از توضیحات ناامنی آن دسته بندی کردیم. با این روش بدون دخالت انسانی می‌توانیم در مقیاس گسترده اقدام به تولید دستورالعمل‌های ناامن کنیم و میزان امنیت مدل را در مقابل آن‌ها بررسی کنیم.

از آنجا که مشاهده کردیم افزودن مدل محافظ و یا استفاده از خودبازتابی و استدلال راه مقاومی برای جلوگیری از شکستن عملکرد است. امید است در آینده با ترکیب روش‌های تولید خودکار دستورالعمل‌های مخرب به همراه یادگیری تقویتی برای آموزش مدل‌های خودبازتابی مانند \lr{DeepSeek R1} و یا \lr{OpenAI o1} منجر به افزایش امنیت مدل‌های زبانی بزرگ شود.
